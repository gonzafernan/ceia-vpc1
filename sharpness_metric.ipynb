{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import YouTubeVideo, display, HTML\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "rcParams[\"animation.embed_limit\"] = 2**128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Sharpness Measure for Blurred Images in Frequency Domain\n",
    "\n",
    "Implementar un detector de máximo enfoque sobre un video aplicando técnicas de análisis espectral similar al que utilizan las cámaras digitales modernas.\n",
    "\n",
    "1. Se debe implementar un algoritmo que dada una imagen, o región, calcule la métrica propuesta en el paper [*Image Sharpness Measure for Blurred Images in Frequency Domain*](https://www.sciencedirect.com/science/article/pii/S1877705813016007) y realizar tres experimentos.\n",
    "    1. Medición sobre todo el frame.\n",
    "    2. Medición sobre una ROI ubicada en el centro del frame. Area de la ROI = 5 o 10% del area total del frame.\n",
    "    3. Medición sobre una matriz de enfoque compuesta por un arreglo de NxM elementos rectangulares equiespaciados. N y M son valores arbitrarios, probar con varios valores 3x3, 7x5, etc.\n",
    "\n",
    "    Para cada experimento se debe presentar:\n",
    "    - Una curva o varias curvas que muestren la evolución de la métrica frame a frame donde se vea claramente cuando el algoritmo detectó el punto de máximo enfoque.\n",
    "    - Video con la ROI o matriz, graficada en rojo y superpuesta al video original para los frames que no están en foco y en verde para los frames donde se detecta la condición de máximo enfoque.\n",
    "\n",
    "2. Calcular la métrica de enfoque eligiendo uno de los algoritmos explicados en el apéndice de: [*Analyze of focus measure operators in shapeform focus*](https://www.researchgate.net/publication/234073157_Analysis_of_focus_measure_operators_in_shape-from-focus).\n",
    "\n",
    "El algoritmo de detección a implementar debe detectar y devolver los puntos de máximo enfoque de manera automática.\n",
    "\n",
    "Extra: Aplicar unsharp masking para expandir la zona de enfoque y devolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUTUBE_VIDEO_ID = \"Nn6EJunyOSI\"\n",
    "video = YouTubeVideo(YOUTUBE_VIDEO_ID, width=700, height=438)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar video a analizar y previsualización de un frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(input_video_path: str) -> list[np.ndarray]:\n",
    "    \"\"\"Get list of frames from a video given its path\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    video_frames = []\n",
    "    # read until is completed\n",
    "    while cap.isOpened():\n",
    "        # capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:  # finished\n",
    "            break\n",
    "        video_frames.append(frame[..., ::-1])\n",
    "    cap.release()\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE_PATH = \"video/focus_video.mov\"\n",
    "video_frames = get_video_frames(VIDEO_FILE_PATH)\n",
    "\n",
    "initial_frame_gray = cv2.cvtColor(video_frames[0], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "frame_fft = np.fft.fft2(initial_frame_gray)\n",
    "frame_fft = np.fft.fftshift(frame_fft)  # low freq to origin for visualization\n",
    "frame_fft = 20 * np.log(np.abs(frame_fft))  # get module\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(12, 10)\n",
    "axs[0].set_title(\"video frame\")\n",
    "axs[0].imshow(video_frames[0])\n",
    "axs[1].set_title(\"video frame grayscale\")\n",
    "axs[1].imshow(initial_frame_gray, cmap=\"gray\")\n",
    "axs[2].set_title(\"video frame fft\")\n",
    "axs[2].imshow(frame_fft, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación del algoritmo que calcula la métrica propuesta en el paper [*Image Sharpness Measure for Blurred Images in Frequency Domain*](https://www.sciencedirect.com/science/article/pii/S1877705813016007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_fm_quality(input_img: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Image quality measure (FM)\n",
    "    Where FM stands for Frequency Domain Image Blur Measure.\n",
    "    \"\"\"\n",
    "    # 1. Compute F which is the Fourier Transform representation of the image I\n",
    "    img_fft = np.fft.fft2(input_img)\n",
    "    # 2. Find Fc which is obtained by shifting the origin of F to centre\n",
    "    img_fft_center = np.fft.fftshift(img_fft)\n",
    "    # 3. Calculate the absolute value of the centered Fourier transform of image I\n",
    "    img_fft_abs = np.abs(img_fft_center)\n",
    "    # 4. Calculate M where M is the maximum value of the frequency component in F\n",
    "    img_fft_max = np.max(img_fft_abs)\n",
    "    # 5. Calculate the total number of pixels in F whose pixel value > thres,\n",
    "    # where thres = M / 1000\n",
    "    img_th = np.count_nonzero(img_fft_abs > img_fft_max / 1000)\n",
    "    # 6. Calculate image quality measure (FM)\n",
    "    return img_th / np.multiply(*input_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De [*Analyze of focus measure operators in shapeform focus*](https://www.researchgate.net/publication/234073157_Analysis_of_focus_measure_operators_in_shape-from-focus), implementación de A.17 *Energy of Laplacian* (LAP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_lap1_quality(input_img: np.ndarray) -> float:\n",
    "    \"\"\"Image quality measure LAP1 (Energy of Laplacian)\"\"\"\n",
    "    lap_energy = np.pow(cv2.Laplacian(src=input_img, ddepth=cv2.CV_16S, ksize=3), 2)\n",
    "    return np.sum(lap_energy) / np.max(lap_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para obtener la calidad por frame de un video (offline) para una métrica genérica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_quality(\n",
    "    input_video_frames: list[np.ndarray],\n",
    "    quality_metric: Callable = get_image_fm_quality,\n",
    ") -> list[float]:\n",
    "    \"\"\"Get video quality for each frame\"\"\"\n",
    "    return [\n",
    "        quality_metric(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "        for frame in input_video_frames\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_max_quality(video_quality: list[float]) -> int:\n",
    "    \"\"\"Get frame index with max FM image quality index\"\"\"\n",
    "    return np.argmax(video_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herramientas para obtener el ROI de la imagen (region of interest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian_tuple = tuple[int, int]  # typedef\n",
    "\n",
    "\n",
    "def get_image_center_roi(\n",
    "    height: int, width: int, roi_percentage: float\n",
    ") -> tuple[cartesian_tuple, cartesian_tuple]:\n",
    "    \"\"\"Get a centered ROI coordinates for a given percentage of the image\"\"\"\n",
    "    roi_width = np.sqrt(width * height * roi_percentage)\n",
    "    roi_upper_left = int(width / 2 - roi_width / 2), int(height / 2 - roi_width / 2)\n",
    "    roi_lower_right = int(width / 2 + roi_width / 2), int(height / 2 + roi_width / 2)\n",
    "    return roi_upper_left, roi_lower_right\n",
    "\n",
    "\n",
    "def get_image_matrix_roi(\n",
    "    height: int, width: int, nrows: int, ncols: int, roi_percentage: float\n",
    ") -> tuple[cartesian_tuple, cartesian_tuple]:\n",
    "    roi_width = np.sqrt(width * height * roi_percentage)\n",
    "    space_factor = 2.0\n",
    "    roi_matrix = []\n",
    "    for i in range(-int(ncols / 2), int(ncols / 2) + 1):\n",
    "        for j in range(-int(nrows / 2), int(nrows / 2) + 1):\n",
    "            roi_upper_left = int(width / 2 - roi_width / 2 + i * space_factor * roi_width), int(\n",
    "                height / 2 - roi_width / 2 + j * space_factor * roi_width\n",
    "            )\n",
    "            roi_lower_right = int(width / 2 + roi_width / 2 + i * space_factor * roi_width), int(\n",
    "                height / 2 + roi_width / 2 + j * space_factor * roi_width\n",
    "            )\n",
    "            roi_matrix.append((roi_upper_left, roi_lower_right))\n",
    "    return roi_matrix\n",
    "\n",
    "\n",
    "def get_image_from_roi(\n",
    "    input_image: np.ndarray,\n",
    "    roi_upper_left: cartesian_tuple,\n",
    "    roi_lower_right: cartesian_tuple,\n",
    "):\n",
    "    return input_image[\n",
    "        roi_upper_left[1] : roi_lower_right[1], roi_upper_left[0] : roi_lower_right[0]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herramientas para anotar las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian_tuple = tuple[int, int]  # typedef\n",
    "\n",
    "FONT_SCALE = 1.8\n",
    "FONT_FACE = cv2.FONT_HERSHEY_PLAIN\n",
    "FONT_THICKNESS = 1\n",
    "\n",
    "\n",
    "def set_annotation_fm_value(\n",
    "    image: np.ndarray,\n",
    "    fm_value: float,\n",
    "    text_color: tuple[int, int, int] = (255, 255, 0),\n",
    "):\n",
    "    \"\"\"Add FM metric text in the image top right corner\"\"\"\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        f\"FM={fm_value:.4f}\",\n",
    "        (450, 50),\n",
    "        fontFace=FONT_FACE,\n",
    "        fontScale=FONT_SCALE,\n",
    "        color=text_color,\n",
    "        thickness=FONT_THICKNESS,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "\n",
    "def set_annotation_roi(\n",
    "    image: np.ndarray,\n",
    "    upper_left: cartesian_tuple,\n",
    "    lower_right: cartesian_tuple,\n",
    "    roi_color: tuple[int, int, int] = (255, 255, 0),\n",
    "):\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        upper_left,\n",
    "        lower_right,\n",
    "        color=roi_color,\n",
    "        thickness=2,\n",
    "        lineType=cv2.LINE_8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medición de enfoque aplicado al frame completo\n",
    "A continuación se puede observar FM y LAP1 aplicado al video en cada frame en su totalidad. Para ambos casos el máximo se encuentra entre el frame 100 y 120. También se observan valores relativamente bajos en FM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_quality_fm = get_video_quality(video_frames, quality_metric=get_image_fm_quality)\n",
    "frame_max_quality_fm = get_max_quality(video_quality_fm)\n",
    "video_quality_lap1 = get_video_quality(\n",
    "    video_frames, quality_metric=get_image_lap1_quality\n",
    ")\n",
    "frame_max_quality_lap1 = get_max_quality(video_quality_lap1)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(18, 3)\n",
    "\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "\n",
    "axs[1].set_title(\"Image quality (FM) on complete frame\")\n",
    "axs[1].axvline(frame_max_quality_fm, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_fm = axs[1].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[1].plot([i for i in range(len(video_frames))], video_quality_fm)\n",
    "\n",
    "axs[2].set_title(\"Image quality (LAP1) on complete frame\")\n",
    "axs[2].axvline(frame_max_quality_lap1, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_lap1 = axs[2].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[2].plot([i for i in range(len(video_frames))], video_quality_lap1)\n",
    "\n",
    "color_red = (255, 0, 0)\n",
    "color_green = (0, 255, 0)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    set_annotation_fm_value(\n",
    "        annotated_frame,\n",
    "        video_quality_fm[i],\n",
    "        color_green if i == frame_max_quality_fm else color_red,\n",
    "    )\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    plot_quality_fm.set_xdata([i, i])\n",
    "    plot_quality_lap1.set_xdata([i, i])\n",
    "    return video_preview, plot_quality_fm, plot_quality_lap1\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, func=update, frames=len(video_frames), interval=30, repeat=False, blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medición de enfoque aplicado a un ROI\n",
    "A continuación se puede observar FM y LAP1 aplicado al video en cada frame en un ROI del 10% centrado. Se puede observar como LAP1 funciona correctamente al aplicarlo al frame completo pero no al utilizarlo en una área limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "pt1, pt2 = get_image_center_roi(video_height, video_width, 0.01)\n",
    "roi_frames = [get_image_from_roi(frame, pt1, pt2) for frame in video_frames]\n",
    "video_quality_fm = get_video_quality(roi_frames, quality_metric=get_image_fm_quality)\n",
    "frame_max_quality_fm = get_max_quality(video_quality_fm)\n",
    "video_quality_lap1 = get_video_quality(\n",
    "    roi_frames, quality_metric=get_image_lap1_quality\n",
    ")\n",
    "frame_max_quality_lap1 = get_max_quality(video_quality_lap1)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(18, 3)\n",
    "\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "\n",
    "axs[1].set_title(\"Image quality (FM) on centered ROI\")\n",
    "axs[1].axvline(frame_max_quality_fm, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_fm = axs[1].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[1].plot([i for i in range(len(video_frames))], video_quality_fm)\n",
    "\n",
    "axs[2].set_title(\"Image quality (LAP1) on centered ROI\")\n",
    "axs[2].axvline(frame_max_quality_lap1, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_lap1 = axs[2].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[2].plot([i for i in range(len(video_frames))], video_quality_lap1)\n",
    "\n",
    "color_red = (255, 0, 0)\n",
    "color_green = (0, 255, 0)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    set_annotation_fm_value(\n",
    "        annotated_frame,\n",
    "        video_quality_fm[i],\n",
    "        color_green if i == frame_max_quality_fm else color_red,\n",
    "    )\n",
    "    set_annotation_roi(\n",
    "        annotated_frame,\n",
    "        pt1,\n",
    "        pt2,\n",
    "        color_green if i == frame_max_quality_fm else color_red,\n",
    "    )\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    plot_quality_fm.set_xdata([i, i])\n",
    "    plot_quality_lap1.set_xdata([i, i])\n",
    "    return video_preview, plot_quality_fm, plot_quality_lap1\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, func=update, frames=len(video_frames), interval=30, repeat=False, blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medición de enfoque aplicado a una matriz de enfoque\n",
    "A continuación se puede observar FM y LAP1 aplicado al video en cada frame en diferentes matrices de enfoque. Las curvas de las métricas para cada frame corresponden al promedio de los valores obtenido en la matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "\n",
    "# ROI matrices to compare\n",
    "roi_matrices = [\n",
    "    get_image_matrix_roi(\n",
    "        height=video_height, width=video_width, nrows=3, ncols=3, roi_percentage=0.001\n",
    "    ),\n",
    "    get_image_matrix_roi(\n",
    "        height=video_height, width=video_width, nrows=3, ncols=7, roi_percentage=0.001\n",
    "    ),\n",
    "    get_image_matrix_roi(\n",
    "        height=video_height, width=video_width, nrows=7, ncols=7, roi_percentage=0.001\n",
    "    ),\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3)\n",
    "fig.set_size_inches(18, 9)\n",
    "axs[0][0].set_title(\"Original video\")\n",
    "axs[0][1].set_title(\"Image quality (FM) on ROI matrix (average)\")\n",
    "axs[0][2].set_title(\"Image quality (LAP1) on ROI matrix (average)\")\n",
    "\n",
    "color_red = (255, 0, 0)\n",
    "color_green = (0, 255, 0)\n",
    "\n",
    "video_quality_fm_roi = []\n",
    "video_quality_lap1_roi = []\n",
    "frame_max_quality_fm_roi = []\n",
    "frame_max_quality_lap1_roi = []\n",
    "\n",
    "video_quality_fm_mean = []\n",
    "\n",
    "# plot artists\n",
    "video_previews = []\n",
    "plots_quality_fm = []\n",
    "plots_quality_lap1 = []\n",
    "\n",
    "for roi_idx, roi_matrix in enumerate(roi_matrices):\n",
    "    video_quality_fm_roi.append([])\n",
    "    video_quality_lap1_roi.append([])\n",
    "    frame_max_quality_fm_roi.append([])\n",
    "    frame_max_quality_lap1_roi.append([])\n",
    "\n",
    "    for roi_upper_left, roi_lower_right in roi_matrix:\n",
    "        roi_frames = [\n",
    "            get_image_from_roi(frame, roi_upper_left, roi_lower_right)\n",
    "            for frame in video_frames\n",
    "        ]\n",
    "        video_quality_fm_roi[-1].append(\n",
    "            get_video_quality(roi_frames, quality_metric=get_image_fm_quality)\n",
    "        )\n",
    "        frame_max_quality_fm_roi[-1].append(get_max_quality(video_quality_fm_roi[-1][-1]))\n",
    "        video_quality_lap1_roi[-1].append(\n",
    "            get_video_quality(roi_frames, quality_metric=get_image_lap1_quality)\n",
    "        )\n",
    "        frame_max_quality_lap1_roi[-1].append(\n",
    "            get_max_quality(video_quality_lap1_roi[-1][-1])\n",
    "        )\n",
    "\n",
    "    # Get image quality average of the matrix\n",
    "    video_quality_fm_mean.append([\n",
    "        np.mean([roi_quality[frame_idx] for roi_quality in video_quality_fm_roi[-1]])\n",
    "        for frame_idx in range(len(video_frames))\n",
    "    ])\n",
    "    video_quality_lap1_mean = [\n",
    "        np.mean([roi_quality[frame_idx] for roi_quality in video_quality_lap1_roi[-1]])\n",
    "        for frame_idx in range(len(video_frames))\n",
    "    ]\n",
    "\n",
    "    video_previews.append(axs[roi_idx][0].imshow(video_frames[0]))\n",
    "\n",
    "    plots_quality_fm.append(\n",
    "        axs[roi_idx][1].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "    )\n",
    "    axs[roi_idx][1].plot([i for i in range(len(video_frames))], video_quality_fm_mean[-1])\n",
    "    axs[roi_idx][1].axvline(\n",
    "        get_max_quality(video_quality_fm_mean[-1]), ls=\"-\", color=\"g\", lw=1, zorder=10\n",
    "    )\n",
    "\n",
    "    plots_quality_lap1.append(\n",
    "        axs[roi_idx][2].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "    )\n",
    "    axs[roi_idx][2].plot([i for i in range(len(video_frames))], video_quality_lap1_mean)\n",
    "    axs[roi_idx][2].axvline(\n",
    "        get_max_quality(video_quality_lap1_mean), ls=\"-\", color=\"g\", lw=1, zorder=10\n",
    "    )\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    for roi_matrix_idx, roi_matrix in enumerate(roi_matrices):\n",
    "        annotated_frame = video_frames[i].copy()\n",
    "        for roi_idx, (roi_upper_left, roi_lower_right) in enumerate(roi_matrix):\n",
    "            set_annotation_roi(\n",
    "                annotated_frame,\n",
    "                roi_upper_left,\n",
    "                roi_lower_right,\n",
    "                (\n",
    "                    color_green\n",
    "                    if i == frame_max_quality_fm_roi[roi_matrix_idx][roi_idx]\n",
    "                    else color_red\n",
    "                ),\n",
    "            )\n",
    "        set_annotation_fm_value(\n",
    "            annotated_frame,\n",
    "            video_quality_fm_mean[roi_matrix_idx][i],\n",
    "            (\n",
    "                color_green\n",
    "                if i in frame_max_quality_fm_roi[roi_matrix_idx]\n",
    "                else color_red\n",
    "            ),\n",
    "        )\n",
    "        video_previews[roi_matrix_idx].set_data(annotated_frame)\n",
    "        plots_quality_fm[roi_matrix_idx].set_xdata([i, i])\n",
    "        plots_quality_lap1[roi_matrix_idx].set_xdata([i, i])\n",
    "    return *video_previews, *plots_quality_fm, *plots_quality_lap1\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, func=update, frames=len(video_frames), interval=30, repeat=False, blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección automática de máximo enfoque\n",
    "A continuación se implementa una herramienta que permite obtener el máximo de enfoque de forma *online*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_red = (255, 0, 0)\n",
    "color_green = (0, 255, 0)\n",
    "\n",
    "\n",
    "class MaxSharpnessDetector:\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_height: int,\n",
    "        video_width: int,\n",
    "        detection_lag: int = 10,\n",
    "        tolerance: float = 0.0025,\n",
    "    ):\n",
    "        self._roi_matrix = get_image_matrix_roi(\n",
    "            height=video_height,\n",
    "            width=video_width,\n",
    "            nrows=7,\n",
    "            ncols=7,\n",
    "            roi_percentage=0.001,\n",
    "        )\n",
    "        self._video_quality_fm_mean = []\n",
    "        self._filt_window_size = detection_lag\n",
    "        self._tolerance = tolerance\n",
    "        self._ma_kernel = np.ones(10) / 10\n",
    "\n",
    "    def update(self, new_frame: np.ndarray):\n",
    "        \"\"\"Give a new video frame to the detector\"\"\"\n",
    "        fm_sum = 0\n",
    "        for roi_upper_left, roi_lower_right in self._roi_matrix:\n",
    "            roi_frame = get_image_from_roi(new_frame, roi_upper_left, roi_lower_right)\n",
    "            fm_sum += get_image_fm_quality(cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY))\n",
    "        self._video_quality_fm_mean.append(fm_sum / len(self._roi_matrix))\n",
    "\n",
    "    def _diff_video_quality(self):\n",
    "        \"\"\"Low pass filter and differentiation of video quality\"\"\"\n",
    "        # can be optimized keeping a sum (moving average)\n",
    "        return np.diff(\n",
    "            np.convolve(self._video_quality_fm_mean, self._ma_kernel, mode=\"valid\")\n",
    "        )\n",
    "\n",
    "    def has_reach_max(self):\n",
    "        # too much noise to perform second derivative\n",
    "        # then, first derivative = 0 and quality metric above threshold\n",
    "        if not self._video_quality_fm_mean[-1] > 0.85:  # check sharp metric is high\n",
    "            return False\n",
    "        return np.abs(self._diff_video_quality()[-1]) < self._tolerance\n",
    "\n",
    "    def get_quality_last_frame(self):\n",
    "        \"\"\"Get quality of the last frame analyzed\"\"\"\n",
    "        return self._video_quality_fm_mean[-1]\n",
    "\n",
    "    def annotate_frame(self, frame: np.ndarray):\n",
    "        for roi_upper_left, roi_lower_right in self._roi_matrix:\n",
    "            set_annotation_roi(\n",
    "                frame,\n",
    "                roi_upper_left,\n",
    "                roi_lower_right,\n",
    "                color_green if self.has_reach_max() else color_red,\n",
    "            )\n",
    "        set_annotation_fm_value(\n",
    "            frame,\n",
    "            self._video_quality_fm_mean[-1],\n",
    "            color_green if self.has_reach_max() else color_red,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación de la herramienta desarrollada sobre el video original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "max_sharpness_detector = MaxSharpnessDetector(video_height, video_width)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "video_preview = ax.imshow(video_frames[0])\n",
    "\n",
    "\n",
    "def next_frame(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    max_sharpness_detector.update(annotated_frame)\n",
    "    max_sharpness_detector.annotate_frame(annotated_frame)\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    return (video_preview,)\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=next_frame,\n",
    "    frames=len(video_frames),\n",
    "    interval=30,\n",
    "    repeat=False,\n",
    "    blit=True,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsharp masking para corrección del desenfoque\n",
    "Implementación de algoritmo de *Unsharp Masking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsharp_masking(image: np.ndarray, alpha: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Unsharp masking\"\"\"\n",
    "    gauss_image = cv2.GaussianBlur(image, (7, 7), 0.5)\n",
    "    return cv2.addWeighted(image, alpha + 1, gauss_image, -alpha, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación del algoritmo en frame desenfocado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(15, 5)\n",
    "axs[0].set_title(\"Original image\")\n",
    "axs[0].imshow(video_frames[0])\n",
    "axs[1].set_title(\"Unsharp masking k=1\")\n",
    "axs[1].imshow(unsharp_masking(video_frames[0]))\n",
    "axs[2].set_title(\"Unsharp masking k=20\")\n",
    "axs[2].imshow(unsharp_masking(video_frames[0], alpha=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "max_sharpness_detector = MaxSharpnessDetector(video_height, video_width)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(10, 5)\n",
    "axs[0].set_title(\"Original video\")\n",
    "axs[1].set_title(\"Unsharp masking\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "sharp_preview = axs[1].imshow(video_frames[0])\n",
    "\n",
    "\n",
    "def next_frame(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    max_sharpness_detector.update(annotated_frame)\n",
    "    max_sharpness_detector.annotate_frame(annotated_frame)\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    unsharp_mask_alpha = 50 * (1 - max_sharpness_detector.get_quality_last_frame())\n",
    "    sharp_frame = unsharp_masking(video_frames[i], alpha=unsharp_mask_alpha)\n",
    "    cv2.putText(\n",
    "        sharp_frame,\n",
    "        f\"k={unsharp_mask_alpha:.4f}\",\n",
    "        (450, 50),\n",
    "        fontFace=FONT_FACE,\n",
    "        fontScale=FONT_SCALE,\n",
    "        color=(255, 255, 0),\n",
    "        thickness=FONT_THICKNESS,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    sharp_preview.set_data(sharp_frame)\n",
    "    return video_preview, sharp_preview\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=next_frame,\n",
    "    frames=len(video_frames),\n",
    "    interval=30,\n",
    "    repeat=False,\n",
    "    blit=True,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

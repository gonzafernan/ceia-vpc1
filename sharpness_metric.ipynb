{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import YouTubeVideo, display, HTML\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "rcParams[\"animation.embed_limit\"] = 2**128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Sharpness Measure for Blurred Images in Frequency Domain\n",
    "\n",
    "Implementar un detector de máximo enfoque sobre un video aplicando técnicas de análisis espectral similar al que utilizan las cámaras digitales modernas.\n",
    "\n",
    "1. Se debe implementar un algoritmo que dada una imagen, o región, calcule la métrica propuesta en el paper [*Image Sharpness Measure for Blurred Images in Frequency Domain*](https://www.sciencedirect.com/science/article/pii/S1877705813016007) y realizar tres experimentos.\n",
    "    1. Medición sobre todo el frame.\n",
    "    2. Medición sobre una ROI ubicada en el centro del frame. Area de la ROI = 5 o 10% del area total del frame.\n",
    "    3. Medición sobre una matriz de enfoque compuesta por un arreglo de NxM elementos rectangulares equiespaciados. N y M son valores arbitrarios, probar con varios valores 3x3, 7x5, etc.\n",
    "\n",
    "    Para cada experimento se debe presentar:\n",
    "    - Una curva o varias curvas que muestren la evolución de la métrica frame a frame donde se vea claramente cuando el algoritmo detectó el punto de máximo enfoque.\n",
    "    - Video con la ROI o matriz, graficada en rojo y superpuesta al video original para los frames que no están en foco y en verde para los frames donde se detecta la condición de máximo enfoque.\n",
    "\n",
    "2. Calcular la métrica de enfoque eligiendo uno de los algoritmos explicados en el apéndice de: [*Analyze of focus measure operators in shapeform focus*](https://www.researchgate.net/publication/234073157_Analysis_of_focus_measure_operators_in_shape-from-focus).\n",
    "\n",
    "El algoritmo de detección a implementar debe detectar y devolver los puntos de máximo enfoque de manera automática.\n",
    "\n",
    "Extra: Aplicar unsharp masking para expandir la zona de enfoque y devolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUTUBE_VIDEO_ID = \"Nn6EJunyOSI\"\n",
    "video = YouTubeVideo(YOUTUBE_VIDEO_ID, width=700, height=438)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar video a analizar y previsualización de un frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(input_video_path: str) -> list[np.ndarray]:\n",
    "    \"\"\"Get list of frames from a video given its path\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    video_frames = []\n",
    "    # read until is completed\n",
    "    while cap.isOpened():\n",
    "        # capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:  # finished\n",
    "            break\n",
    "        video_frames.append(frame[..., ::-1])\n",
    "    cap.release()\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE_PATH = \"video/focus_video.mov\"\n",
    "video_frames = get_video_frames(VIDEO_FILE_PATH)\n",
    "\n",
    "initial_frame_gray = cv2.cvtColor(video_frames[0], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "frame_fft = np.fft.fft2(initial_frame_gray)\n",
    "frame_fft = np.fft.fftshift(frame_fft)  # low freq to origin for visualization\n",
    "frame_fft = 20 * np.log(np.abs(frame_fft))  # get module\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(12, 10)\n",
    "axs[0].set_title(\"video frame\")\n",
    "axs[0].imshow(video_frames[0])\n",
    "axs[1].set_title(\"video frame grayscale\")\n",
    "axs[1].imshow(initial_frame_gray, cmap=\"gray\")\n",
    "axs[2].set_title(\"video frame fft\")\n",
    "axs[2].imshow(frame_fft, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación del algoritmo que calcula la métrica propuesta en el paper [*Image Sharpness Measure for Blurred Images in Frequency Domain*](https://www.sciencedirect.com/science/article/pii/S1877705813016007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_fm_quality(input_img: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Image quality measure (FM)\n",
    "    Where FM stands for Frequency Domain Image Blur Measure.\n",
    "    \"\"\"\n",
    "    # 1. Compute F which is the Fourier Transform representation of the image I\n",
    "    img_fft = np.fft.fft2(input_img)\n",
    "    # 2. Find Fc which is obtained by shifting the origin of F to centre\n",
    "    img_fft_center = np.fft.fftshift(img_fft)\n",
    "    # 3. Calculate the absolute value of the centered Fourier transform of image I\n",
    "    img_fft_abs = np.abs(img_fft_center)\n",
    "    # 4. Calculate M where M is the maximum value of the frequency component in F\n",
    "    img_fft_max = np.max(img_fft_abs)\n",
    "    # 5. Calculate the total number of pixels in F whose pixel value > thres,\n",
    "    # where thres = M / 1000\n",
    "    img_th = np.count_nonzero(img_fft_abs > img_fft_max / 1000)\n",
    "    # 6. Calculate image quality measure (FM)\n",
    "    return img_th / np.multiply(*input_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_lap1_quality(input_img: np.ndarray) -> float:\n",
    "    \"\"\"Image quality measure LAP1 (Energy of Laplacian)\"\"\"\n",
    "    lap_energy = np.pow(cv2.Laplacian(src=input_img, ddepth=cv2.CV_16S, ksize=3), 2)\n",
    "    return np.sum(lap_energy) / np.max(lap_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_quality(\n",
    "    input_video_frames: list[np.ndarray],\n",
    "    quality_metric: Callable = get_image_fm_quality,\n",
    ") -> list[float]:\n",
    "    \"\"\"Get video quality for each frame\"\"\"\n",
    "    return [\n",
    "        quality_metric(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "        for frame in input_video_frames\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_max_quality(video_quality: list[float]) -> int:\n",
    "    \"\"\"Get frame index with max FM image quality index\"\"\"\n",
    "    return np.argmax(video_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herramienta para obtener el ROI de la imagen (region of interest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian_tuple = tuple[int, int]  # typedef\n",
    "\n",
    "\n",
    "def get_image_center_roi(\n",
    "    height: int, width: int, roi_percentage: float\n",
    ") -> tuple[cartesian_tuple, cartesian_tuple]:\n",
    "    \"\"\"Get a centered ROI coordinates for a given percentage of the image\"\"\"\n",
    "    roi_width = np.sqrt(width * height * roi_percentage)\n",
    "    roi_upper_left = int(width / 2 - roi_width / 2), int(height / 2 - roi_width / 2)\n",
    "    roi_lower_right = int(width / 2 + roi_width / 2), int(height / 2 + roi_width / 2)\n",
    "    return roi_upper_left, roi_lower_right\n",
    "\n",
    "\n",
    "def get_image_matrix_roi(\n",
    "    height: int, width: int, nrows: int, ncols: int, roi_percentage: float\n",
    ") -> tuple[cartesian_tuple, cartesian_tuple]:\n",
    "    roi_width = np.sqrt(width * height * roi_percentage)\n",
    "    space_factor = 2.0\n",
    "    roi_matrix = []\n",
    "    for i in range(-int(ncols / 2), int(ncols / 2) + 1):\n",
    "        for j in range(-int(nrows / 2), int(nrows / 2) + 1):\n",
    "            roi_upper_left = int(width / 2 - roi_width / 2 + i * space_factor * roi_width), int(\n",
    "                height / 2 - roi_width / 2 + j * space_factor * roi_width\n",
    "            )\n",
    "            roi_lower_right = int(width / 2 + roi_width / 2 + i * space_factor * roi_width), int(\n",
    "                height / 2 + roi_width / 2 + j * space_factor * roi_width\n",
    "            )\n",
    "            roi_matrix.append((roi_upper_left, roi_lower_right))\n",
    "    return roi_matrix\n",
    "\n",
    "\n",
    "def get_image_from_roi(\n",
    "    input_image: np.ndarray,\n",
    "    roi_upper_left: cartesian_tuple,\n",
    "    roi_lower_right: cartesian_tuple,\n",
    "):\n",
    "    return input_image[\n",
    "        roi_upper_left[1] : roi_lower_right[1], roi_upper_left[0] : roi_lower_right[0]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herramientas para anotar las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian_tuple = tuple[int, int]  # typedef\n",
    "\n",
    "FONT_SCALE = 1.8\n",
    "FONT_FACE = cv2.FONT_HERSHEY_PLAIN\n",
    "FONT_THICKNESS = 1\n",
    "\n",
    "\n",
    "def set_annotation_fm_value(\n",
    "    image: np.ndarray,\n",
    "    fm_value: float,\n",
    "    text_color: tuple[int, int, int] = (255, 255, 0),\n",
    "):\n",
    "    \"\"\"Add FM metric text in the image top right corner\"\"\"\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        f\"FM={fm_value:.4f}\",\n",
    "        (450, 50),\n",
    "        fontFace=FONT_FACE,\n",
    "        fontScale=FONT_SCALE,\n",
    "        color=text_color,\n",
    "        thickness=FONT_THICKNESS,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "\n",
    "def set_annotation_roi(\n",
    "    image: np.ndarray,\n",
    "    upper_left: cartesian_tuple,\n",
    "    lower_right: cartesian_tuple,\n",
    "    roi_color: tuple[int, int, int] = (255, 255, 0),\n",
    "):\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        upper_left,\n",
    "        lower_right,\n",
    "        color=roi_color,\n",
    "        thickness=2,\n",
    "        lineType=cv2.LINE_8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sharpness measure applied to the complete frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_quality_fm = get_video_quality(video_frames, quality_metric=get_image_fm_quality)\n",
    "frame_max_quality_fm = get_max_quality(video_quality_fm)\n",
    "video_quality_lap1 = get_video_quality(\n",
    "    video_frames, quality_metric=get_image_lap1_quality\n",
    ")\n",
    "frame_max_quality_lap1 = get_max_quality(video_quality_lap1)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(18, 3)\n",
    "\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "\n",
    "axs[1].set_title(\"Image quality (FM) on complete frame\")\n",
    "axs[1].axvline(frame_max_quality_fm, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_fm = axs[1].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[1].plot([i for i in range(len(video_frames))], video_quality_fm)\n",
    "\n",
    "axs[2].set_title(\"Image quality (LAP1) on complete frame\")\n",
    "axs[2].axvline(frame_max_quality_lap1, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_lap1 = axs[2].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[2].plot([i for i in range(len(video_frames))], video_quality_lap1)\n",
    "\n",
    "color_red = (255, 0, 0)\n",
    "color_green = (0, 255, 0)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    set_annotation_fm_value(\n",
    "        annotated_frame,\n",
    "        video_quality_fm[i],\n",
    "        color_green if i == frame_max_quality_fm else color_red,\n",
    "    )\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    plot_quality_fm.set_xdata([i, i])\n",
    "    plot_quality_lap1.set_xdata([i, i])\n",
    "    return video_preview, plot_quality_fm, plot_quality_lap1\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, func=update, frames=len(video_frames), interval=30, repeat=False, blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sharpness measure in Frequency Domain applied to a ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "pt1, pt2 = get_image_center_roi(video_height, video_width, 0.01)\n",
    "roi_frames = [get_image_from_roi(frame, pt1, pt2) for frame in video_frames]\n",
    "video_quality_fm = get_video_quality(roi_frames, quality_metric=get_image_fm_quality)\n",
    "frame_max_quality_fm = get_max_quality(video_quality_fm)\n",
    "video_quality_lap1 = get_video_quality(\n",
    "    roi_frames, quality_metric=get_image_lap1_quality\n",
    ")\n",
    "frame_max_quality_lap1 = get_max_quality(video_quality_lap1)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(18, 3)\n",
    "\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "\n",
    "axs[1].set_title(\"Image quality (FM) on centered ROI\")\n",
    "axs[1].axvline(frame_max_quality_fm, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_fm = axs[1].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[1].plot([i for i in range(len(video_frames))], video_quality_fm)\n",
    "\n",
    "axs[2].set_title(\"Image quality (LAP1) on centered ROI\")\n",
    "axs[2].axvline(frame_max_quality_lap1, ls=\"-\", color=\"g\", lw=1, zorder=10)\n",
    "plot_quality_lap1 = axs[2].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[2].plot([i for i in range(len(video_frames))], video_quality_lap1)\n",
    "\n",
    "color_red = (255, 0, 0)\n",
    "color_green = (0, 255, 0)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    set_annotation_fm_value(\n",
    "        annotated_frame,\n",
    "        video_quality_fm[i],\n",
    "        color_green if i == frame_max_quality_fm else color_red,\n",
    "    )\n",
    "    set_annotation_roi(\n",
    "        annotated_frame,\n",
    "        pt1,\n",
    "        pt2,\n",
    "        color_green if i == frame_max_quality_fm else color_red,\n",
    "    )\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    plot_quality_fm.set_xdata([i, i])\n",
    "    plot_quality_lap1.set_xdata([i, i])\n",
    "    return video_preview, plot_quality_fm, plot_quality_lap1\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, func=update, frames=len(video_frames), interval=30, repeat=False, blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sharpness measure in Frequency Domain applied to a ROI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "\n",
    "roi_matrix = get_image_matrix_roi(\n",
    "    height=video_height, width=video_width, nrows=3, ncols=3, roi_percentage=0.001\n",
    ")\n",
    "\n",
    "video_quality_fm_roi = []\n",
    "video_quality_lap1_roi = []\n",
    "frame_max_quality_fm_roi = []\n",
    "frame_max_quality_lap1_roi = []\n",
    "for roi_upper_left, roi_lower_right in roi_matrix:\n",
    "    roi_frames = [\n",
    "        get_image_from_roi(frame, roi_upper_left, roi_lower_right)\n",
    "        for frame in video_frames\n",
    "    ]\n",
    "    video_quality_fm_roi.append(\n",
    "        get_video_quality(roi_frames, quality_metric=get_image_fm_quality)\n",
    "    )\n",
    "    frame_max_quality_fm_roi.append(get_max_quality(video_quality_fm_roi[-1]))\n",
    "    video_quality_lap1_roi.append(\n",
    "        get_video_quality(roi_frames, quality_metric=get_image_lap1_quality)\n",
    "    )\n",
    "    frame_max_quality_lap1_roi.append(get_max_quality(video_quality_lap1_roi[-1]))\n",
    "\n",
    "# Get image quality average of the matrix\n",
    "video_quality_fm_mean = [\n",
    "    np.mean([roi_quality[frame_idx] for roi_quality in video_quality_fm_roi])\n",
    "    for frame_idx in range(len(video_frames))\n",
    "]\n",
    "video_quality_lap1_mean = [\n",
    "    np.mean([roi_quality[frame_idx] for roi_quality in video_quality_lap1_roi])\n",
    "    for frame_idx in range(len(video_frames))\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(18, 3)\n",
    "\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "\n",
    "axs[1].set_title(\"Image quality (FM) on ROI matrix (average)\")\n",
    "plot_quality_fm = axs[1].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[1].plot([i for i in range(len(video_frames))], video_quality_fm_mean)\n",
    "axs[1].axvline(\n",
    "    get_max_quality(video_quality_fm_mean), ls=\"-\", color=\"g\", lw=1, zorder=10\n",
    ")\n",
    "\n",
    "axs[2].set_title(\"Image quality (LAP1) on ROI matrix (average)\")\n",
    "plot_quality_lap1 = axs[2].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "axs[2].plot([i for i in range(len(video_frames))], video_quality_lap1_mean)\n",
    "axs[2].axvline(\n",
    "    get_max_quality(video_quality_lap1_mean), ls=\"-\", color=\"g\", lw=1, zorder=10\n",
    ")\n",
    "\n",
    "color_red = (255, 0, 0)\n",
    "color_green = (0, 255, 0)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    for roi_idx, (roi_upper_left, roi_lower_right) in enumerate(roi_matrix):\n",
    "        set_annotation_roi(\n",
    "            annotated_frame,\n",
    "            roi_upper_left,\n",
    "            roi_lower_right,\n",
    "            color_green if i == frame_max_quality_fm_roi[roi_idx] else color_red,\n",
    "        )\n",
    "    set_annotation_fm_value(\n",
    "        annotated_frame,\n",
    "        video_quality_fm[i],\n",
    "        color_green if i in frame_max_quality_fm_roi else color_red,\n",
    "    )\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    plot_quality_fm.set_xdata([i, i])\n",
    "    plot_quality_lap1.set_xdata([i, i])\n",
    "    return video_preview, plot_quality_fm, plot_quality_lap1\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, func=update, frames=len(video_frames), interval=30, repeat=False, blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

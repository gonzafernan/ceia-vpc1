{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import YouTubeVideo, display, HTML\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Sharpness Measure for Blurred Images in Frequency Domain\n",
    "\n",
    "Implementar un detector de máximo enfoque sobre un video aplicando técnicas de análisis espectral similar al que utilizan las cámaras digitales modernas.\n",
    "\n",
    "1. Se debe implementar un algoritmo que dada una imagen, o región, calcule la métrica propuesta en el paper [*Image Sharpness Measure for Blurred Images in Frequency Domain*](https://www.sciencedirect.com/science/article/pii/S1877705813016007) y realizar tres experimentos.\n",
    "    1. Medición sobre todo el frame.\n",
    "    2. Medición sobre una ROI ubicada en el centro del frame. Area de la ROI = 5 o 10% del area total del frame.\n",
    "    3. Medición sobre una matriz de enfoque compuesta por un arreglo de NxM elementos rectangulares equiespaciados. N y M son valores arbitrarios, probar con varios valores 3x3, 7x5, etc.\n",
    "\n",
    "    Para cada experimento se debe presentar:\n",
    "    - Una curva o varias curvas que muestren la evolución de la métrica frame a frame donde se vea claramente cuando el algoritmo detectó el punto de máximo enfoque.\n",
    "    - Video con la ROI o matriz, graficada en rojo y superpuesta al video original para los frames que no están en foco y en verde para los frames donde se detecta la condición de máximo enfoque.\n",
    "\n",
    "2. Calcular la métrica de enfoque eligiendo uno de los algoritmos explicados en el apéndice de: *Analyze of focus measure operators in shapeform focus*.\n",
    "\n",
    "El algoritmo de detección a implementar debe detectar y devolver los puntos de máximo enfoque de manera automática.\n",
    "\n",
    "Extra: Aplicar unsharp masking para expandir la zona de enfoque y devolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUTUBE_VIDEO_ID = \"Nn6EJunyOSI\"\n",
    "video = YouTubeVideo(YOUTUBE_VIDEO_ID, width=700, height=438)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(input_video_path: str) -> list[np.ndarray]:\n",
    "    \"\"\"Get list of frames from a video given its path\"\"\"\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    video_frames = []\n",
    "    # read until is completed\n",
    "    while cap.isOpened():\n",
    "        # capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:  # finished\n",
    "            break\n",
    "        video_frames.append(frame[..., ::-1])\n",
    "    cap.release()\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE_PATH = \"video/focus_video.mov\"\n",
    "video_frames = get_video_frames(VIDEO_FILE_PATH)\n",
    "\n",
    "initial_frame_gray = cv2.cvtColor(video_frames[0], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "frame_fft = np.fft.fft2(initial_frame_gray)\n",
    "frame_fft = np.fft.fftshift(frame_fft)  # low freq to origin for visualization\n",
    "frame_fft = 20 * np.log(np.abs(frame_fft))  # get module\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(12, 10)\n",
    "axs[0].set_title(\"video frame\")\n",
    "axs[0].imshow(video_frames[0])\n",
    "axs[1].set_title(\"video frame grayscale\")\n",
    "axs[1].imshow(initial_frame_gray, cmap=\"gray\")\n",
    "axs[2].set_title(\"video frame fft\")\n",
    "axs[2].imshow(frame_fft, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación del algoritmo que calcula la métrica propuesta en el paper [*Image Sharpness Measure for Blurred Images in Frequency Domain*](https://www.sciencedirect.com/science/article/pii/S1877705813016007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_fm_quality(input_img: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Image quality measure (FM)\n",
    "    Where FM stands for Frequency Domain Image Blur Measure.\n",
    "    \"\"\"\n",
    "    # 1. Compute F which is the Fourier Transform representation of the image I\n",
    "    img_fft = np.fft.fft2(input_img)\n",
    "    # 2. Find Fc which is obtained by shifting the origin of F to centre\n",
    "    img_fft_center = np.fft.fftshift(img_fft)\n",
    "    # 3. Calculate the absolute value of the centered Fourier transform of image I\n",
    "    img_fft_abs = np.abs(img_fft_center)\n",
    "    # 4. Calculate M where M is the maximum value of the frequency component in F\n",
    "    img_fft_max = np.max(img_fft_abs)\n",
    "    # 5. Calculate the total number of pixels in F whose pixel value > thres,\n",
    "    # where thres = M / 1000\n",
    "    img_th = np.count_nonzero(img_fft_abs > img_fft_max / 1000)\n",
    "    # 6. Calculate image quality measure (FM)\n",
    "    return img_th / np.multiply(*input_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_fm_quality(input_video_frames: list[np.ndarray]):\n",
    "    \"\"\"\n",
    "    Video quality measure (FM)\n",
    "    Where FM stands for Frequency Domain Image Blur Measure.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        get_image_fm_quality(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "        for frame in input_video_frames\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herramienta para obtener el ROI de la imagen (region of interest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian_tuple = tuple[int, int]  # typedef\n",
    "\n",
    "\n",
    "def get_image_center_roi(\n",
    "    height: int, width: int, roi_percentage: float\n",
    ") -> tuple[cartesian_tuple, cartesian_tuple]:\n",
    "    \"\"\"Get a centered ROI coordinates for a given percentage of the image\"\"\"\n",
    "    roi_width = np.sqrt(width * height * roi_percentage)\n",
    "    roi_upper_left = int(width / 2 - roi_width / 2), int(height / 2 - roi_width / 2)\n",
    "    roi_lower_right = int(width / 2 + roi_width / 2), int(height / 2 + roi_width / 2)\n",
    "    return roi_upper_left, roi_lower_right\n",
    "\n",
    "\n",
    "def get_image_matrix_roi(\n",
    "    height: int, width: int, nrows: int, ncols: int, roi_percentage: float\n",
    ") -> tuple[cartesian_tuple, cartesian_tuple]:\n",
    "    roi_width = np.sqrt(width * height * roi_percentage)\n",
    "    space_factor = 2.0\n",
    "    roi_matrix = []\n",
    "    for i in range(-int(ncols / 2), int(ncols / 2) + 1):\n",
    "        for j in range(-int(nrows / 2), int(nrows / 2) + 1):\n",
    "            roi_upper_left = int(width / 2 - roi_width / 2 + i * space_factor * roi_width), int(\n",
    "                height / 2 - roi_width / 2 + j * space_factor * roi_width\n",
    "            )\n",
    "            roi_lower_right = int(width / 2 + roi_width / 2 + i * space_factor * roi_width), int(\n",
    "                height / 2 + roi_width / 2 + j * space_factor * roi_width\n",
    "            )\n",
    "            roi_matrix.append((roi_upper_left, roi_lower_right))\n",
    "    return roi_matrix\n",
    "\n",
    "\n",
    "def get_image_from_roi(\n",
    "    input_image: np.ndarray,\n",
    "    roi_upper_left: cartesian_tuple,\n",
    "    roi_lower_right: cartesian_tuple,\n",
    "):\n",
    "    return input_image[\n",
    "        roi_upper_left[0] : roi_lower_right[0], roi_upper_left[1] : roi_lower_right[1]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sharpness measure in Frequency Domain applied to the complete frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_quality_fm = get_video_fm_quality(video_frames)\n",
    "\n",
    "rcParams['animation.embed_limit'] = 2**128\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 3)\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "axs[1].set_title(\"Image quality (FM) on complete frame\")\n",
    "time_tracker = axs[1].axvline(0, ls='-', color='r', lw=1, zorder=10)\n",
    "axs[1].plot([i for i in range(len(video_frames))], video_quality_fm)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    video_preview.set_data(video_frames[i])\n",
    "    time_tracker.set_xdata([i, i])\n",
    "    return video_preview, time_tracker\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=update,\n",
    "    frames=len(video_frames),\n",
    "    interval=30,\n",
    "    repeat=False,\n",
    "    blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sharpness measure in Frequency Domain applied to a ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "pt1, pt2 = get_image_center_roi(video_height, video_width, 0.01)\n",
    "roi_frames = [get_image_from_roi(frame, pt1, pt2) for frame in video_frames]\n",
    "video_quality_fm = get_video_fm_quality(roi_frames)\n",
    "\n",
    "rcParams['animation.embed_limit'] = 2**128\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 3)\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "axs[1].set_title(\"Image quality (FM) on centered ROI\")\n",
    "time_tracker = axs[1].axvline(0, ls='-', color='r', lw=1, zorder=10)\n",
    "axs[1].plot([i for i in range(len(video_frames))], video_quality_fm)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    cv2.rectangle(\n",
    "        annotated_frame,\n",
    "        pt1,\n",
    "        pt2,\n",
    "        color=(255, 0, 0),\n",
    "        thickness=2,\n",
    "        lineType=cv2.LINE_8,\n",
    "    )\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    time_tracker.set_xdata([i, i])\n",
    "    return video_preview, time_tracker\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=update,\n",
    "    frames=len(video_frames),\n",
    "    interval=30,\n",
    "    repeat=False,\n",
    "    blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sharpness measure in Frequency Domain applied to a ROI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = get_video_frames(input_video_path=VIDEO_FILE_PATH)\n",
    "video_height, video_width, _ = video_frames[0].shape\n",
    "video_quality_fm_roi = []\n",
    "roi_matrix = get_image_matrix_roi(\n",
    "    height=video_height, width=video_width, nrows=3, ncols=7, roi_percentage=0.001\n",
    ")\n",
    "for roi_upper_left, roi_lower_right in roi_matrix:\n",
    "    roi_frames = [\n",
    "        get_image_from_roi(frame, roi_upper_left, roi_lower_right)\n",
    "        for frame in video_frames\n",
    "    ]\n",
    "    # video_quality_fm_roi.append(get_video_fm_quality(roi_frames))\n",
    "\n",
    "rcParams[\"animation.embed_limit\"] = 2**128\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 3)\n",
    "axs[0].set_title(\"Original video\")\n",
    "video_preview = axs[0].imshow(video_frames[0])\n",
    "axs[1].set_title(\"Image quality (FM) on centered ROI\")\n",
    "time_tracker = axs[1].axvline(0, ls=\"-\", color=\"r\", lw=1, zorder=10)\n",
    "for video_quality_fm in video_quality_fm_roi:\n",
    "    axs[1].plot([i for i in range(len(video_frames))], video_quality_fm)\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    annotated_frame = video_frames[i].copy()\n",
    "    for roi_upper_left, roi_lower_right in roi_matrix:\n",
    "        cv2.rectangle(\n",
    "            annotated_frame,\n",
    "            roi_upper_left,\n",
    "            roi_lower_right,\n",
    "            color=(255, 0, 0),\n",
    "            thickness=2,\n",
    "            lineType=cv2.LINE_8,\n",
    "        )\n",
    "    video_preview.set_data(annotated_frame)\n",
    "    time_tracker.set_xdata([i, i])\n",
    "    return video_preview, time_tracker\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig, func=update, frames=len(video_frames), interval=30, repeat=False, blit=True\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
